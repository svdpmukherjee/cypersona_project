{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e6b7ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22d14423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "RAW_DATA_DIR = Path(\"data/raw_data\")\n",
    "CLEANED_DATA_DIR = Path(\"data/cleaned_data\")\n",
    "FEATURES_DIR = Path(\"data/features\")\n",
    "CLEANED_DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "FEATURES_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8318cf3a",
   "metadata": {},
   "source": [
    "## WASH et al. (2021) Dataset Processing\n",
    "### Step 1: Cleaning with renaming readable feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad8439d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wash2021Processor:\n",
    "    \n",
    "    @staticmethod\n",
    "    def process():\n",
    "        print(\"Processing WASH 2021 Dataset...\")\n",
    "        \n",
    "        # Load raw data\n",
    "        df = pd.read_csv(\"data/raw_data/phishing_wash_2021.csv\")\n",
    "        print(f\"Raw: {len(df)} rows, {len(df.columns)} columns\")\n",
    "        \n",
    "        # Complete feature mapping including qualitative columns\n",
    "        features = {\n",
    "            # Demographics & Background\n",
    "            'age': 'age',\n",
    "            'gender': 'gender', \n",
    "            'gender_other': 'gender_3_TEXT',\n",
    "            'ethnicity': 'ethnicity',\n",
    "            'ethnicity_other': 'ethnicity_9_TEXT',\n",
    "            'education_level': 'education',\n",
    "            'employment_status': 'employment',\n",
    "            'annual_income': 'income',\n",
    "            'has_it_training': 'expert_training',\n",
    "            'has_it_job': 'expert_job',\n",
    "            'can_recall_phishing': 'recall_email',\n",
    "            \n",
    "            # Digital Literacy Scale\n",
    "            'digital_literacy_wiki': 'digital_literacy_1',\n",
    "            'digital_literacy_meme': 'digital_literacy_2',\n",
    "            'digital_literacy_phishing': 'digital_literacy_3',\n",
    "            'digital_literacy_bookmark': 'digital_literacy_4',\n",
    "            'digital_literacy_cache': 'digital_literacy_5',\n",
    "            'digital_literacy_ssl': 'digital_literacy_6',\n",
    "            'digital_literacy_ajax': 'digital_literacy_7',\n",
    "            'digital_literacy_rss': 'digital_literacy_8',\n",
    "            'digital_literacy_other': 'digital_literacy_9',\n",
    "            \n",
    "            # QUALITATIVE ELICITATIONS\n",
    "            'unsafe_email_ways_list': 'elicitation1',\n",
    "            'suspicious_email_recognition': 'elicitation2', \n",
    "            'suspicious_emails_received': 'elicitation3',\n",
    "            \n",
    "            # EMAIL SELECTION & BRIEF DESCRIPTIONS\n",
    "            'email_brief_summary': 'brief_summary',\n",
    "            'what_made_email_suspicious': 'describe_suspicious',\n",
    "            'what_made_decision_hard': 'describe_hard',\n",
    "            'what_email_asked_todo': 'describe_ask',\n",
    "            \n",
    "            # Emotional Responses\n",
    "            'emotion_dread': 'emotions_1',\n",
    "            'emotion_terror': 'emotions_2',\n",
    "            'emotion_anxiety': 'emotions_3',\n",
    "            'emotion_nervous': 'emotions_4',\n",
    "            'emotion_scared': 'emotions_5',\n",
    "            'emotion_panic': 'emotions_6',\n",
    "            'emotion_fear': 'emotions_7',\n",
    "            'emotion_worry': 'emotions_8',\n",
    "            \n",
    "            # NOTICING: What They Noticed About The Email\n",
    "            'email_features_noticed': 'notice1',\n",
    "            'email_recency': 'notice2',\n",
    "            'email_account_type': 'notice3',\n",
    "            'email_account_type_other': 'notice3_4_TEXT',\n",
    "            'email_content_type': 'notice4',\n",
    "            'email_content_type_other': 'notice4_3_TEXT',\n",
    "            'email_sender_type': 'notice5',\n",
    "            'email_sender_type_other': 'notice5_5_TEXT',\n",
    "            \n",
    "            # EXPECTING: Context and Expectations\n",
    "            'felt_similar_before': 'expect1',\n",
    "            'previous_sender_emails': 'expect2',\n",
    "            'previous_sender_interaction': 'expect3',\n",
    "            'sender_relationship_duration': 'expect4',\n",
    "            'expected_this_email': 'expect5',\n",
    "            'email_seemed_different': 'expect6',\n",
    "            \n",
    "            # SUSPECTING: What Made Them Suspicious\n",
    "            'actions_requested': 'suspect1',\n",
    "            'sender_issues': 'suspect2',\n",
    "            'subject_line_issues': 'suspect3',\n",
    "            'email_body_issues': 'suspect4',\n",
    "            'overall_suspicion': 'suspect5',\n",
    "            'suspicion_confidence': 'suspect5_sure_1',\n",
    "            \n",
    "            # INVESTIGATING: How They Investigated\n",
    "            'investigation_methods': 'investigate1',\n",
    "            'investigation_methods_other': 'investigate1_9_TEXT',\n",
    "            'contacted_sender_how': 'investigate2',\n",
    "            \n",
    "            # DECIDING: Decision Process and Actions\n",
    "            'final_decision': 'decide',\n",
    "            'decision_confidence': 'decide_sure_1',\n",
    "            'actions_with_email': 'act',\n",
    "            \n",
    "            # HARM & FULL STORY\n",
    "            'perceived_harm': 'harm',\n",
    "            'detailed_incident_narrative': 'full_story',\n",
    "            'story_recall_difficulty': 'full_story_easy',\n",
    "            \n",
    "            # Cybersecurity History\n",
    "            'previous_incidents': 'victim'\n",
    "        }\n",
    "        \n",
    "        # Create cleaned dataframe\n",
    "        cleaned = pd.DataFrame()\n",
    "        missing_cols = []\n",
    "        \n",
    "        for new_col, old_col in features.items():\n",
    "            if old_col in df.columns:\n",
    "                cleaned[new_col] = df[old_col]\n",
    "            else:\n",
    "                print(f\"Warning: Column '{old_col}' not found, setting '{new_col}' to NaN\")\n",
    "                cleaned[new_col] = np.nan\n",
    "                missing_cols.append(old_col)\n",
    "        \n",
    "        print(f\"Initial mapping: {len(cleaned)} rows, {len(cleaned.columns)} features\")\n",
    "        \n",
    "        initial_count = len(cleaned)\n",
    "\n",
    "        # Filter 1: Must be able to recall phishing emails\n",
    "        if 'can_recall_phishing' in cleaned.columns:\n",
    "            cleaned = cleaned[cleaned['can_recall_phishing'].str.contains('Yes', case=False, na=False)]\n",
    "            print(f\"After phishing recall filter: {len(cleaned)} rows (removed {initial_count - len(cleaned)})\")\n",
    "        \n",
    "        # Filter 2: Keep only participants who responded to final decision\n",
    "        if 'final_decision' in cleaned.columns:\n",
    "            cleaned = cleaned[cleaned['final_decision'].notna() & (cleaned['final_decision'].str.strip() != '')]\n",
    "            print(f\"After final decision filter: {len(cleaned)} rows (removed {initial_count - len(cleaned)})\")\n",
    "\n",
    "        for col in cleaned.select_dtypes(include=['object']).columns:\n",
    "            cleaned[col] = cleaned[col].str.replace(r'[\\r\\n]+', ' ', regex=True).str.strip()\n",
    "        \n",
    "            \n",
    "        return cleaned\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf6b9f",
   "metadata": {},
   "source": [
    "### Step 2. Feature Engineering\n",
    "\n",
    "##### **Purpose**: Extract behavioral decision patterns from real phishing encounters\n",
    "\n",
    "##### **Unique Value**: Real-world authentic phishing incidents with genuine emotional and behavioral responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaab4d8",
   "metadata": {},
   "source": [
    "##### **Research Questions Enabled**:\n",
    "1. \"What personal factors predict risky vs safe email decisions?\"\n",
    "2. \"How do emotions influence phishing susceptibility?\" \n",
    "3. \"Which investigation behaviors correlate with accurate decisions?\"\n",
    "4. \"Can we predict decision confidence from contextual factors?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a2182f",
   "metadata": {},
   "source": [
    "#### **X_inputs**:\n",
    "1. Demographics (5): Age, gender, education, employment, income\n",
    "2. Cybersecurity background (7): Previous incidents, IT training/job status  \n",
    "3. Digital Literacy Scale (10): Technical knowledge across 9 domains + composite score\n",
    "4. Emotional Response Profile (9): Fear, anxiety, panic, worry etc. + composite score\n",
    "5. Investigation Behaviors (3): How they verified sender, links, external sources\n",
    "6. Email Context & Characteristics (16): Recency, account type, content type, sender relationship\n",
    "7. Confidence & Risk Perception (3): Suspicion levels, confidence scores to detect phishing, perceived harm to phishing incidents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b9236f",
   "metadata": {},
   "source": [
    "#### **Y_targets**:\n",
    "1. Primary Decision: Safe vs Unsafe classification of phishing incident (binary)\n",
    "2. Decision Confidence: How certain they were about the classification (0-10 scale, standardized)\n",
    "3. Behavioral Actions: Clicked links, reported spam, deleted, ignored (4 binary indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e8ca027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ml_optimized(df):\n",
    "    \"\"\"\n",
    "    Transform cleaned WASH data into ML-ready features with proper encodings\n",
    "    based on actual data values and ML model feature requirements\n",
    "    \"\"\"\n",
    "    \n",
    "    ml_df = pd.DataFrame()\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    print(\"Creating ML-optimized WASH features...\")\n",
    "    print(f\"Input data: {len(df)} rows, {len(df.columns)} columns\")\n",
    "    \n",
    "    # ===================================================================\n",
    "    # 1. DEMOGRAPHICS & BACKGROUND (5 features)\n",
    "    # ===================================================================\n",
    "    \n",
    "    # Age categories\n",
    "    age_bins = [0, 25, 35, 55, 75, 100]\n",
    "    age_labels = [1, 2, 3, 4, 5]  # 1=youngest, 5=oldest\n",
    "    age_cat = pd.cut(df['age'].astype(float), bins=age_bins, labels=age_labels, include_lowest=True)\n",
    "    ml_df['age_category'] = age_cat.astype(float).fillna(3) \n",
    "    \n",
    "    # Gender encoding (binary only)\n",
    "    gender_map = {'Man': 1, 'Woman': 0}\n",
    "    ml_df['gender'] = df['gender'].map(gender_map).fillna(0) \n",
    "    \n",
    "    # Education level\n",
    "    education_map = {\n",
    "        # Low Education (1): Below high school\n",
    "        'None, or grades 1-8': 1, \n",
    "        'Some high school': 1,\n",
    "        \n",
    "        # Medium-Low Education (2): High school / Trade school\n",
    "        'High school graduate or GED certificate': 2,\n",
    "        'Technical, trade, or vocational school AFTER high school': 2, \n",
    "        \n",
    "        # Medium-High Education (3): Some college / Bachelor's\n",
    "        'Some college, no 4-year degree': 3,\n",
    "        '4-year college degree': 3,\n",
    "        \n",
    "        # High Education (4): Graduate/Professional degree\n",
    "        'Some postgraduate or professional schooling, no postgraduate degree': 4,\n",
    "        \"Postgraduate or professional degree, including master's, doctorate, medical or law degree\": 4\n",
    "    }\n",
    "    ml_df['education_level'] = df['education_level'].map(education_map).fillna(2) \n",
    "    \n",
    "    # Employment status (binary: employed vs not employed)\n",
    "    employment_map = {\n",
    "        'Employed full time': 1, \n",
    "        'Employed part time': 1,\n",
    "        'Unemployed looking for work': 0,\n",
    "        'Unemployed not looking for work': 0, \n",
    "        'Retired': 0,\n",
    "        'Student': 0 \n",
    "    }\n",
    "    ml_df['employment_status'] = df['employment_status'].map(employment_map).fillna(0)\n",
    "    \n",
    "    # Annual income\n",
    "    income_map = {\n",
    "        # Low Income (1):\n",
    "        'Less than $25,000': 1, \n",
    "        '$25,000 to $34,999': 1,\n",
    "        \n",
    "        # Lower-Middle Income (2):\n",
    "        '$35,000 to $49,999': 2,\n",
    "        '$50,000 to $74,999': 2, \n",
    "        \n",
    "        # Upper-Middle Income (3):\n",
    "        '$75,000 to $99,999': 3,\n",
    "        '$100,000 to $149,999': 3,\n",
    "        \n",
    "        # High Income (4):\n",
    "        '$150,000 to $199,999': 4,\n",
    "        '$200,000 or more': 4\n",
    "    }\n",
    "    ml_df['annual_income'] = df['annual_income'].map(income_map).fillna(2)  # Default to lower-middle\n",
    "    \n",
    "    # ===================================================================\n",
    "    # 2. IT BACKGROUND & SECURITY HISTORY (3 features)\n",
    "    # ===================================================================\n",
    "    \n",
    "    # IT training and job status (binary)\n",
    "    ml_df['has_it_training'] = (df['has_it_training'] == 'Yes').astype(int)\n",
    "    ml_df['has_it_job'] = (df['has_it_job'] == 'Yes').astype(int)\n",
    "    \n",
    "    # Previous incidents - ONE-HOT ENCODED for different incident types\n",
    "    if 'previous_incidents' in df.columns:\n",
    "        ml_df['previous_incidents_phishing_email'] = df['previous_incidents'].str.contains(\n",
    "            'Fell victim to a phishing email message or other scam email', na=False\n",
    "        ).astype(int)\n",
    "        ml_df['previous_incidents_data_breach'] = df['previous_incidents'].str.contains(\n",
    "            'Received a notification from a company that your information was involved in a data breach', na=False\n",
    "        ).astype(int)\n",
    "        ml_df['previous_incidents_computer_virus'] = df['previous_incidents'].str.contains(\n",
    "            'Had a virus on your computer or mobile device', na=False\n",
    "        ).astype(int)\n",
    "        ml_df['previous_incidents_device_hacked'] = df['previous_incidents'].str.contains(\n",
    "            'Someone broke in or hacked your computer, mobile device, or account', na=False\n",
    "        ).astype(int)\n",
    "        ml_df['previous_incidents_credit_card_fraud'] = df['previous_incidents'].str.contains(\n",
    "            'Stranger used your credit card number without your knowledge or permission', na=False\n",
    "        ).astype(int)\n",
    "        ml_df['previous_incidents_identity_theft'] = df['previous_incidents'].str.contains(\n",
    "            'Identity theft more extensive than use of your credit card number without permission', na=False\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Overall indicator: any security incident (excluding \"None of the above\")\n",
    "        ml_df['previous_incidents_any'] = (~df['previous_incidents'].str.contains('None of the above', na=True)).astype(int)\n",
    "    else:\n",
    "        ml_df['previous_incidents_phishing_email'] = 0\n",
    "        ml_df['previous_incidents_data_breach'] = 0\n",
    "        ml_df['previous_incidents_computer_virus'] = 0\n",
    "        ml_df['previous_incidents_device_hacked'] = 0\n",
    "        ml_df['previous_incidents_credit_card_fraud'] = 0\n",
    "        ml_df['previous_incidents_identity_theft'] = 0\n",
    "        ml_df['previous_incidents_any'] = 0\n",
    "    \n",
    "    # ===================================================================\n",
    "    # 3. DIGITAL LITERACY (10 features) \n",
    "    # ===================================================================\n",
    "    \n",
    "    # Digital literacy scale: None=1, Little=2, Some=3, Good=4, Full=5\n",
    "    literacy_map = {'None': 1, 'Little': 2, 'Some': 3, 'Good': 4, 'Full': 5}\n",
    "    \n",
    "    literacy_fields = [\n",
    "        'digital_literacy_wiki', 'digital_literacy_meme', 'digital_literacy_phishing',\n",
    "        'digital_literacy_bookmark', 'digital_literacy_cache', 'digital_literacy_ssl',\n",
    "        'digital_literacy_ajax', 'digital_literacy_rss', 'digital_literacy_other'\n",
    "    ]\n",
    "    \n",
    "    for field in literacy_fields:\n",
    "        if field in df.columns:\n",
    "            encoded_vals = df[field].map(literacy_map).fillna(1) \n",
    "            ml_df[field] = pd.Series(\n",
    "                scaler.fit_transform(encoded_vals.values.reshape(-1, 1)).flatten(),\n",
    "                index=df.index\n",
    "            )\n",
    "        else:\n",
    "            # Create placeholder if missing\n",
    "            ml_df[field] = 0.0\n",
    "    \n",
    "    # Digital literacy total score (average of all components)\n",
    "    literacy_cols = [field for field in literacy_fields if field in ml_df.columns]\n",
    "    if literacy_cols:\n",
    "        ml_df['digital_literacy_total'] = ml_df[literacy_cols].mean(axis=1)\n",
    "    else:\n",
    "        ml_df['digital_literacy_total'] = 0.0\n",
    "    \n",
    "    # ===================================================================\n",
    "    # 4. EMOTIONAL RESPONSE (9 features)\n",
    "    # ===================================================================\n",
    "    \n",
    "    # Emotion scale: Not at all=1, Somewhat=2, Moderately=3, Quite a bit=4, An extreme amount=5\n",
    "    emotion_map = {\n",
    "        'Not at all': 1, \n",
    "        'Somewhat': 2, \n",
    "        'Moderately': 3, \n",
    "        'Quite a bit': 4, \n",
    "        'An extreme amount': 5\n",
    "    }\n",
    "    \n",
    "    emotion_fields = [\n",
    "        'emotion_dread', 'emotion_terror', 'emotion_anxiety', 'emotion_nervous',\n",
    "        'emotion_scared', 'emotion_panic', 'emotion_fear', 'emotion_worry'\n",
    "    ]\n",
    "    \n",
    "    for field in emotion_fields:\n",
    "        if field in df.columns:\n",
    "            encoded_vals = df[field].map(emotion_map).fillna(1)  # Default to \"Not at all\"\n",
    "            ml_df[field] = pd.Series(\n",
    "                scaler.fit_transform(encoded_vals.values.reshape(-1, 1)).flatten(),\n",
    "                index=df.index\n",
    "            )\n",
    "        else:\n",
    "            ml_df[field] = 0.0\n",
    "    \n",
    "    # Emotion total score (average emotional intensity)\n",
    "    emotion_cols = [field for field in emotion_fields if field in ml_df.columns]\n",
    "    if emotion_cols:\n",
    "        ml_df['emotion_total'] = ml_df[emotion_cols].mean(axis=1)\n",
    "    else:\n",
    "        ml_df['emotion_total'] = 0.0\n",
    "    \n",
    "    # ===================================================================\n",
    "    # 5. INVESTIGATION BEHAVIORS (3 features)\n",
    "    # ===================================================================\n",
    "    \n",
    "    # Parse investigation methods (multi-select field)\n",
    "    if 'investigation_methods' in df.columns:\n",
    "        ml_df['investigated_sender'] = df['investigation_methods'].str.contains(\n",
    "            'Looked more closely at the the email address|Asked someone else', na=False\n",
    "        ).astype(int)\n",
    "        \n",
    "        ml_df['investigated_links'] = df['investigation_methods'].str.contains(\n",
    "            'Hovered over|Clicked on one or more of the links', na=False\n",
    "        ).astype(int)\n",
    "        \n",
    "        ml_df['investigated_external'] = df['investigation_methods'].str.contains(\n",
    "            'Looked at email headers|Opened the attachment', na=False\n",
    "        ).astype(int)\n",
    "    else:\n",
    "        ml_df['investigated_sender'] = 0\n",
    "        ml_df['investigated_links'] = 0\n",
    "        ml_df['investigated_external'] = 0\n",
    "    \n",
    "    # ===================================================================\n",
    "    # 6. EMAIL CONTEXT & CHARACTERISTICS (12+ features with one-hot encoding)\n",
    "    # ===================================================================\n",
    "    \n",
    "    # Email recency (1=most recent to 5=oldest)\n",
    "    recency_map = {\n",
    "        'Within the last day': 1,\n",
    "        'Within the last week': 2, \n",
    "        'Within the last month': 3,\n",
    "        'Within the last year': 4,\n",
    "        'Longer than one year ago': 5\n",
    "    }\n",
    "    ml_df['email_recency'] = df['email_recency'].map(recency_map).fillna(3) \n",
    "    \n",
    "    # Email account type - ONE-HOT ENCODED\n",
    "    if 'email_account_type' in df.columns:\n",
    "        ml_df['email_account_work'] = (df['email_account_type'] == 'Work Email account').astype(int)\n",
    "        ml_df['email_account_student'] = (df['email_account_type'] == 'Student Email account').astype(int)\n",
    "        ml_df['email_account_personal'] = (df['email_account_type'] == 'Personal Email account').astype(int)\n",
    "    else:\n",
    "        ml_df['email_account_work'] = 0\n",
    "        ml_df['email_account_student'] = 0\n",
    "        ml_df['email_account_personal'] = 1  \n",
    "    \n",
    "    # Email content type - ONE-HOT ENCODED\n",
    "    if 'email_content_type' in df.columns:\n",
    "        ml_df['email_content_work_related'] = (df['email_content_type'] == 'This email was related to work').astype(int)\n",
    "        ml_df['email_content_personal'] = (df['email_content_type'] == 'This email was of a personal nature').astype(int)\n",
    "    else:\n",
    "        ml_df['email_content_work_related'] = 0\n",
    "        ml_df['email_content_personal'] = 1  \n",
    "    \n",
    "    # Email sender type - ONE-HOT ENCODED\n",
    "    if 'email_sender_type' in df.columns:\n",
    "        ml_df['email_sender_work_colleague'] = (df['email_sender_type'] == 'A work colleague').astype(int)\n",
    "        ml_df['email_sender_friend_family'] = (df['email_sender_type'] == 'A close friend or family member').astype(int)\n",
    "        ml_df['email_sender_acquaintance'] = (df['email_sender_type'] == 'An acquaintance from outside work').astype(int)\n",
    "        ml_df['email_sender_organization'] = (df['email_sender_type'] == 'A company, business or other organization').astype(int)\n",
    "    else:\n",
    "        ml_df['email_sender_work_colleague'] = 0\n",
    "        ml_df['email_sender_friend_family'] = 0\n",
    "        ml_df['email_sender_acquaintance'] = 0\n",
    "        ml_df['email_sender_organization'] = 1 \n",
    "    \n",
    "    # Sender relationship duration (ordinal scale)\n",
    "    duration_map = {\n",
    "        'One month or less': 1,\n",
    "        'Between one month and one year': 2,\n",
    "        'One to two years': 3,\n",
    "        'Two to five years': 4,\n",
    "        'Five to ten years': 5,\n",
    "        'More than 10 years': 6\n",
    "    }\n",
    "    ml_df['sender_relationship_duration'] = df['sender_relationship_duration'].map(duration_map).fillna(1)\n",
    "    \n",
    "    # Expected this email (binary)\n",
    "    expected_map = {'Yes': 1, 'No': 0, \"I'm not sure\": 0}\n",
    "    ml_df['expected_this_email'] = df['expected_this_email'].map(expected_map).fillna(0)\n",
    "    \n",
    "    # Felt similar before (Likert scale 1-5)\n",
    "    likert_map = {\n",
    "        'Strongly disagree': 1, \n",
    "        'Somewhat disagree': 2, \n",
    "        'Neither agree nor disagree': 3,\n",
    "        'Somewhat agree': 4, \n",
    "        'Strongly agree': 5\n",
    "    }\n",
    "    ml_df['felt_similar_before'] = df['felt_similar_before'].map(likert_map).fillna(3)\n",
    "    \n",
    "    # Previous sender emails (binary)\n",
    "    if 'previous_sender_emails' in df.columns:\n",
    "        ml_df['previous_sender_emails'] = (df['previous_sender_emails'] == 'Yes').astype(int)\n",
    "    else:\n",
    "        ml_df['previous_sender_emails'] = 0\n",
    "    \n",
    "    # Previous sender interaction (binary) \n",
    "    if 'previous_sender_interaction' in df.columns:\n",
    "        ml_df['previous_sender_interaction'] = (df['previous_sender_interaction'] == 'Yes').astype(int)\n",
    "    else:\n",
    "        ml_df['previous_sender_interaction'] = 0\n",
    "    \n",
    "    # Email seemed different (Likert scale 1-5)\n",
    "    if 'email_seemed_different' in df.columns:\n",
    "        ml_df['email_seemed_different'] = df['email_seemed_different'].map(likert_map).fillna(3)\n",
    "    else:\n",
    "        ml_df['email_seemed_different'] = 3\n",
    "    \n",
    "    # Parse noticed features (binary indicators)\n",
    "    if 'email_features_noticed' in df.columns:\n",
    "        ml_df['noticed_sender_issues'] = df['email_features_noticed'].str.contains(\n",
    "            \"Sender's name\", na=False\n",
    "        ).astype(int)\n",
    "        \n",
    "        ml_df['noticed_content_issues'] = df['email_features_noticed'].str.contains(\n",
    "            'What the email was about|Length of the email|Information missing', na=False\n",
    "        ).astype(int)\n",
    "        \n",
    "        ml_df['noticed_technical_issues'] = df['email_features_noticed'].str.contains(\n",
    "            'Link|Formatting|Mistakes|File', na=False\n",
    "        ).astype(int)\n",
    "    else:\n",
    "        ml_df['noticed_sender_issues'] = 0\n",
    "        ml_df['noticed_content_issues'] = 0\n",
    "        ml_df['noticed_technical_issues'] = 0\n",
    "    \n",
    "    # Actions requested (multi-select, one-hot encoded)\n",
    "    if 'actions_requested' in df.columns:\n",
    "        ml_df['actions_requested_click_link'] = df['actions_requested'].str.contains(\n",
    "            'Click on a link or button', na=False\n",
    "        ).astype(int)\n",
    "        ml_df['actions_requested_open_attachment'] = df['actions_requested'].str.contains(\n",
    "            'Open something that was attached to the email', na=False\n",
    "        ).astype(int)\n",
    "        ml_df['actions_requested_respond_info'] = df['actions_requested'].str.contains(\n",
    "            'Respond to the email with some information', na=False\n",
    "        ).astype(int)\n",
    "        ml_df['actions_requested_external_action'] = df['actions_requested'].str.contains(\n",
    "            'Take some action outside of the email', na=False\n",
    "        ).astype(int)\n",
    "    else:\n",
    "        ml_df['actions_requested_click_link'] = 0\n",
    "        ml_df['actions_requested_open_attachment'] = 0\n",
    "        ml_df['actions_requested_respond_info'] = 0\n",
    "        ml_df['actions_requested_external_action'] = 0\n",
    "    \n",
    "    # Sender issues (one-hot encoded)\n",
    "    if 'sender_issues' in df.columns:\n",
    "        ml_df['sender_issues_none'] = df['sender_issues'].str.contains(\n",
    "            \"I didn't notice anything that felt off about the sender\", na=False\n",
    "        ).astype(int)\n",
    "        ml_df['sender_issues_name_different'] = df['sender_issues'].str.contains(\n",
    "            \"The sender's name looked different than I would expect\", na=False\n",
    "        ).astype(int)\n",
    "        ml_df['sender_issues_email_different'] = df['sender_issues'].str.contains(\n",
    "            \"The sender's email address looked different than I would expect\", na=False\n",
    "        ).astype(int)\n",
    "    else:\n",
    "        ml_df['sender_issues_none'] = 1\n",
    "        ml_df['sender_issues_name_different'] = 0\n",
    "        ml_df['sender_issues_email_different'] = 0\n",
    "    \n",
    "    # Subject line issues (one-hot encoded)\n",
    "    if 'subject_line_issues' in df.columns:\n",
    "        ml_df['subject_line_issues_none'] = df['subject_line_issues'].str.contains(\n",
    "            \"I didn't notice anything that felt off about the subject line\", na=False\n",
    "        ).astype(int)\n",
    "        ml_df['subject_line_issues_different'] = df['subject_line_issues'].str.contains(\n",
    "            \"The subject line was different than I would expect\", na=False\n",
    "        ).astype(int)\n",
    "    else:\n",
    "        ml_df['subject_line_issues_none'] = 1\n",
    "        ml_df['subject_line_issues_different'] = 0\n",
    "    \n",
    "    # Email body issues (multi-select, one-hot encoded)\n",
    "    if 'email_body_issues' in df.columns:\n",
    "        ml_df['email_body_issues_none'] = df['email_body_issues'].str.contains(\n",
    "            \"I didn't notice anything that felt off about the main body of the email\", na=False\n",
    "        ).astype(int)\n",
    "        ml_df['email_body_issues_typos'] = df['email_body_issues'].str.contains(\n",
    "            'The main body of the email included typos or other issues', na=False\n",
    "        ).astype(int)\n",
    "        ml_df['email_body_issues_missing'] = df['email_body_issues'].str.contains(\n",
    "            'The main body of the email was missing something', na=False\n",
    "        ).astype(int)\n",
    "        ml_df['email_body_issues_strange'] = df['email_body_issues'].str.contains(\n",
    "            'The main body of the email included something strange', na=False\n",
    "        ).astype(int)\n",
    "        ml_df['email_body_issues_more_info'] = df['email_body_issues'].str.contains(\n",
    "            'The main body of the email included more information than I expect', na=False\n",
    "        ).astype(int)\n",
    "        ml_df['email_body_issues_less_info'] = df['email_body_issues'].str.contains(\n",
    "            'The main body of the email included less information than I expect', na=False\n",
    "        ).astype(int)\n",
    "    else:\n",
    "        ml_df['email_body_issues_none'] = 1\n",
    "        ml_df['email_body_issues_typos'] = 0\n",
    "        ml_df['email_body_issues_missing'] = 0\n",
    "        ml_df['email_body_issues_strange'] = 0\n",
    "        ml_df['email_body_issues_more_info'] = 0\n",
    "        ml_df['email_body_issues_less_info'] = 0\n",
    "    \n",
    "    # ===================================================================\n",
    "    # 7. CONFIDENCE & PERCEPTION (3 features)\n",
    "    # ===================================================================\n",
    "    \n",
    "    # Confidence scores: Original scale 1-11, normalize to 0-10 scale then standardize\n",
    "    if 'suspicion_confidence' in df.columns:\n",
    "\n",
    "        suspicion_conf = pd.to_numeric(df['suspicion_confidence'], errors='coerce')\n",
    "        suspicion_conf_normalized = (suspicion_conf - 1).clip(0, 10).fillna(4.5)\n",
    "        ml_df['suspicion_confidence'] = pd.Series(\n",
    "            scaler.fit_transform(suspicion_conf_normalized.values.reshape(-1, 1)).flatten(),\n",
    "            index=df.index\n",
    "        )\n",
    "    else:\n",
    "        ml_df['suspicion_confidence'] = 0.0\n",
    "    \n",
    "    # Overall suspicion (binary: clear yes/no only)\n",
    "    suspicion_map = {\n",
    "        'No, I did not think it was harmful': 0,\n",
    "        'Yes, I thought it was harmful': 1\n",
    "    }\n",
    "    ml_df['overall_suspicion'] = df['overall_suspicion'].map(suspicion_map).fillna(0)\n",
    "    \n",
    "    # Perceived harm (Likert scale 1-5, standardized)\n",
    "    if 'perceived_harm' in df.columns:\n",
    "        harm_encoded = df['perceived_harm'].map(likert_map).fillna(3) \n",
    "        ml_df['perceived_harm'] = pd.Series(\n",
    "            scaler.fit_transform(harm_encoded.values.reshape(-1, 1)).flatten(),\n",
    "            index=df.index\n",
    "        )\n",
    "    else:\n",
    "        ml_df['perceived_harm'] = 0.0\n",
    "    \n",
    "    # ===================================================================\n",
    "    # 8. TARGET VARIABLES - DECISION OUTCOMES (6 features)\n",
    "    # ===================================================================\n",
    "    \n",
    "    # Final decision (primary target - binary: clear safe/unsafe only)\n",
    "    decision_map = {\n",
    "        'Yes, the email was safe': 1,\n",
    "        'No, the email was definitely not safe': 0\n",
    "    }\n",
    "    ml_df['final_decision'] = df['final_decision'].map(decision_map).fillna(0)\n",
    "    \n",
    "    # Decision confidence (normalize 1-11 to 0-10 scale)\n",
    "    if 'decision_confidence' in df.columns:\n",
    "\n",
    "        decision_conf = pd.to_numeric(df['decision_confidence'], errors='coerce')\n",
    "        decision_conf_normalized = (decision_conf - 1).clip(0, 10).fillna(4.5)\n",
    "        ml_df['decision_confidence'] = pd.Series(\n",
    "            scaler.fit_transform(decision_conf_normalized.values.reshape(-1, 1)).flatten(),\n",
    "            index=df.index\n",
    "        )\n",
    "    else:\n",
    "        ml_df['decision_confidence'] = 0.0\n",
    "    \n",
    "    # Parse actions taken (binary indicators for each action type)\n",
    "    if 'actions_with_email' in df.columns:\n",
    "        ml_df['actions_taken_clicked'] = df['actions_with_email'].str.contains(\n",
    "            'clicked|Clicked', na=False\n",
    "        ).astype(int)\n",
    "        \n",
    "        ml_df['actions_taken_reported'] = df['actions_with_email'].str.contains(\n",
    "            'report.*spam|Clicked a button to report', na=False\n",
    "        ).astype(int)\n",
    "        \n",
    "        ml_df['actions_taken_deleted'] = df['actions_with_email'].str.contains(\n",
    "            'Deleted', na=False\n",
    "        ).astype(int)\n",
    "        \n",
    "        ml_df['actions_taken_ignored'] = df['actions_with_email'].str.contains(\n",
    "            'Left.*inbox', na=False\n",
    "        ).astype(int)\n",
    "    else:\n",
    "        ml_df['actions_taken_clicked'] = 0\n",
    "        ml_df['actions_taken_reported'] = 0\n",
    "        ml_df['actions_taken_deleted'] = 0\n",
    "        ml_df['actions_taken_ignored'] = 0\n",
    "    \n",
    "    # Ensure all columns are numeric \n",
    "    for col in ml_df.columns:\n",
    "        if ml_df[col].dtype == 'object' or pd.api.types.is_categorical_dtype(ml_df[col]):\n",
    "            ml_df[col] = pd.to_numeric(ml_df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Final fill for any remaining NaN values - now safe since all columns are numeric\n",
    "    ml_df = ml_df.fillna(0)\n",
    "    \n",
    "    print(f\"ML features created: {len(ml_df)} rows, {len(ml_df.columns)} features\")\n",
    "    print(f\"Feature columns: {list(ml_df.columns)}\")\n",
    "    \n",
    "    return ml_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48d0b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_column_suffixes(df, suffixes=['_encoded', '_binary', '_standardized']):\n",
    "    \"\"\"Remove specified suffixes from column names\"\"\"\n",
    "    new_columns = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        new_col = col\n",
    "        for suffix in suffixes:\n",
    "            if col.endswith(suffix):\n",
    "                new_col = col.replace(suffix, '')\n",
    "                break\n",
    "        new_columns[col] = new_col\n",
    "    \n",
    "    return df.rename(columns=new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d7e4038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WASH 2021 PHISHING BEHAVIOR DATASET PROCESSOR\n",
      "Processing WASH 2021 Dataset...\n",
      "Raw: 1099 rows, 137 columns\n",
      "Initial mapping: 1099 rows, 65 features\n",
      "After phishing recall filter: 476 rows (removed 623)\n",
      "After final decision filter: 325 rows (removed 774)\n",
      "Creating ML-optimized WASH features...\n",
      "Input data: 325 rows, 65 columns\n",
      "ML features created: 325 rows, 79 features\n",
      "Feature columns: ['age_category', 'gender', 'education_level', 'employment_status', 'annual_income', 'has_it_training', 'has_it_job', 'previous_incidents_phishing_email', 'previous_incidents_data_breach', 'previous_incidents_computer_virus', 'previous_incidents_device_hacked', 'previous_incidents_credit_card_fraud', 'previous_incidents_identity_theft', 'previous_incidents_any', 'digital_literacy_wiki', 'digital_literacy_meme', 'digital_literacy_phishing', 'digital_literacy_bookmark', 'digital_literacy_cache', 'digital_literacy_ssl', 'digital_literacy_ajax', 'digital_literacy_rss', 'digital_literacy_other', 'digital_literacy_total', 'emotion_dread', 'emotion_terror', 'emotion_anxiety', 'emotion_nervous', 'emotion_scared', 'emotion_panic', 'emotion_fear', 'emotion_worry', 'emotion_total', 'investigated_sender', 'investigated_links', 'investigated_external', 'email_recency', 'email_account_work', 'email_account_student', 'email_account_personal', 'email_content_work_related', 'email_content_personal', 'email_sender_work_colleague', 'email_sender_friend_family', 'email_sender_acquaintance', 'email_sender_organization', 'sender_relationship_duration', 'expected_this_email', 'felt_similar_before', 'previous_sender_emails', 'previous_sender_interaction', 'email_seemed_different', 'noticed_sender_issues', 'noticed_content_issues', 'noticed_technical_issues', 'actions_requested_click_link', 'actions_requested_open_attachment', 'actions_requested_respond_info', 'actions_requested_external_action', 'sender_issues_none', 'sender_issues_name_different', 'sender_issues_email_different', 'subject_line_issues_none', 'subject_line_issues_different', 'email_body_issues_none', 'email_body_issues_typos', 'email_body_issues_missing', 'email_body_issues_strange', 'email_body_issues_more_info', 'email_body_issues_less_info', 'suspicion_confidence', 'overall_suspicion', 'perceived_harm', 'final_decision', 'decision_confidence', 'actions_taken_clicked', 'actions_taken_reported', 'actions_taken_deleted', 'actions_taken_ignored']\n",
      "\n",
      "Cleaned dataset: 325 rows, 65 columns\n",
      "ML dataset: 325 rows, 79 features\n",
      "Saved to: data/cleaned_data and data/features\n"
     ]
    }
   ],
   "source": [
    "# === MAIN PROCESSING ===\n",
    "print(\"WASH 2021 PHISHING BEHAVIOR DATASET PROCESSOR\")\n",
    "\n",
    "# Process data\n",
    "wash_cleaned = Wash2021Processor.process()\n",
    "wash_ml = create_ml_optimized(wash_cleaned)\n",
    "\n",
    "# Save datasets\n",
    "wash_cleaned.to_csv(CLEANED_DATA_DIR / \"wash_2021_cleaned.csv\", index=False)\n",
    "wash_ml_clean = remove_column_suffixes(wash_ml)\n",
    "wash_ml_clean.to_csv(FEATURES_DIR / \"wash_2021_ml_optimized.csv\", index=False)\n",
    "\n",
    "print(f\"\\nCleaned dataset: {len(wash_cleaned)} rows, {len(wash_cleaned.columns)} columns\")\n",
    "print(f\"ML dataset: {len(wash_ml)} rows, {len(wash_ml.columns)} features\")\n",
    "print(f\"Saved to: {CLEANED_DATA_DIR} and {FEATURES_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d155e901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 73, Samples: 325\n",
      "final_decision | RF | ACCURACY=0.9815\n",
      "final_decision | LR_OR_RIDGE | ACCURACY=0.9538\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        64\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.98        65\n",
      "   macro avg       0.49      0.50      0.50        65\n",
      "weighted avg       0.97      0.98      0.98        65\n",
      "\n",
      "actions_taken_clicked | RF | ACCURACY=0.6862\n",
      "actions_taken_clicked | LR_OR_RIDGE | ACCURACY=0.5846\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.96      0.83        45\n",
      "           1       0.67      0.20      0.31        20\n",
      "\n",
      "    accuracy                           0.72        65\n",
      "   macro avg       0.70      0.58      0.57        65\n",
      "weighted avg       0.71      0.72      0.67        65\n",
      "\n",
      "actions_taken_reported | RF | ACCURACY=0.6862\n",
      "actions_taken_reported | LR_OR_RIDGE | ACCURACY=0.5846\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.96      0.83        45\n",
      "           1       0.67      0.20      0.31        20\n",
      "\n",
      "    accuracy                           0.72        65\n",
      "   macro avg       0.70      0.58      0.57        65\n",
      "weighted avg       0.71      0.72      0.67        65\n",
      "\n",
      "actions_taken_deleted | RF | ACCURACY=0.7631\n",
      "actions_taken_deleted | LR_OR_RIDGE | ACCURACY=0.6123\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        15\n",
      "           1       0.76      0.96      0.85        50\n",
      "\n",
      "    accuracy                           0.74        65\n",
      "   macro avg       0.38      0.48      0.42        65\n",
      "weighted avg       0.59      0.74      0.65        65\n",
      "\n",
      "actions_taken_ignored | RF | ACCURACY=0.9631\n",
      "actions_taken_ignored | LR_OR_RIDGE | ACCURACY=0.8923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        63\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.97        65\n",
      "   macro avg       0.48      0.50      0.49        65\n",
      "weighted avg       0.94      0.97      0.95        65\n",
      "\n",
      "decision_confidence | RF | R2=0.3371\n",
      "decision_confidence | LR_OR_RIDGE | R2=0.1467\n",
      "Test R²: 0.3177, RMSE: 0.8502\n",
      "\n",
      "=== Top Feature Importances for Each Model ===\n",
      "\n",
      "--- final_decision ---\n",
      "                      feature  importance\n",
      "20      digital_literacy_ajax    0.082017\n",
      "70       suspicion_confidence    0.071147\n",
      "32              emotion_total    0.063842\n",
      "16  digital_literacy_phishing    0.053087\n",
      "36              email_recency    0.042638\n",
      "28             emotion_scared    0.037350\n",
      "29              emotion_panic    0.037014\n",
      "23     digital_literacy_total    0.036994\n",
      "27            emotion_nervous    0.034843\n",
      "34         investigated_links    0.030965\n",
      "0                age_category    0.025808\n",
      "31              emotion_worry    0.024653\n",
      "21       digital_literacy_rss    0.024623\n",
      "19       digital_literacy_ssl    0.023376\n",
      "26            emotion_anxiety    0.023037\n",
      "\n",
      "--- actions_taken_clicked ---\n",
      "                              feature  importance\n",
      "23             digital_literacy_total    0.081882\n",
      "32                      emotion_total    0.039634\n",
      "14              digital_literacy_wiki    0.033378\n",
      "70               suspicion_confidence    0.031416\n",
      "17          digital_literacy_bookmark    0.026170\n",
      "15              digital_literacy_meme    0.025360\n",
      "4                       annual_income    0.024654\n",
      "72                     perceived_harm    0.023072\n",
      "48                felt_similar_before    0.022087\n",
      "18             digital_literacy_cache    0.022040\n",
      "58  actions_requested_external_action    0.021975\n",
      "36                      email_recency    0.021781\n",
      "0                        age_category    0.020377\n",
      "19               digital_literacy_ssl    0.020297\n",
      "16          digital_literacy_phishing    0.020195\n",
      "\n",
      "--- actions_taken_reported ---\n",
      "                              feature  importance\n",
      "23             digital_literacy_total    0.081882\n",
      "32                      emotion_total    0.039634\n",
      "14              digital_literacy_wiki    0.033378\n",
      "70               suspicion_confidence    0.031416\n",
      "17          digital_literacy_bookmark    0.026170\n",
      "15              digital_literacy_meme    0.025360\n",
      "4                       annual_income    0.024654\n",
      "72                     perceived_harm    0.023072\n",
      "48                felt_similar_before    0.022087\n",
      "18             digital_literacy_cache    0.022040\n",
      "58  actions_requested_external_action    0.021975\n",
      "36                      email_recency    0.021781\n",
      "0                        age_category    0.020377\n",
      "19               digital_literacy_ssl    0.020297\n",
      "16          digital_literacy_phishing    0.020195\n",
      "\n",
      "--- actions_taken_deleted ---\n",
      "                              feature  importance\n",
      "23             digital_literacy_total    0.054970\n",
      "32                      emotion_total    0.054269\n",
      "55       actions_requested_click_link    0.034039\n",
      "30                       emotion_fear    0.031605\n",
      "70               suspicion_confidence    0.028329\n",
      "18             digital_literacy_cache    0.026020\n",
      "16          digital_literacy_phishing    0.023999\n",
      "72                     perceived_harm    0.023669\n",
      "28                     emotion_scared    0.023524\n",
      "2                     education_level    0.023363\n",
      "19               digital_literacy_ssl    0.022964\n",
      "0                        age_category    0.022766\n",
      "36                      email_recency    0.022498\n",
      "4                       annual_income    0.021775\n",
      "9   previous_incidents_computer_virus    0.020837\n",
      "\n",
      "--- actions_taken_ignored ---\n",
      "                   feature  importance\n",
      "32           emotion_total    0.077746\n",
      "48     felt_similar_before    0.061399\n",
      "23  digital_literacy_total    0.056031\n",
      "29           emotion_panic    0.047091\n",
      "31           emotion_worry    0.042491\n",
      "28          emotion_scared    0.041633\n",
      "26         emotion_anxiety    0.031385\n",
      "15   digital_literacy_meme    0.030906\n",
      "36           email_recency    0.029183\n",
      "14   digital_literacy_wiki    0.027022\n",
      "18  digital_literacy_cache    0.025732\n",
      "51  email_seemed_different    0.025481\n",
      "70    suspicion_confidence    0.025452\n",
      "27         emotion_nervous    0.025010\n",
      "71       overall_suspicion    0.021536\n",
      "\n",
      "--- decision_confidence ---\n",
      "                      feature  importance\n",
      "70       suspicion_confidence    0.506957\n",
      "4               annual_income    0.035682\n",
      "23     digital_literacy_total    0.034613\n",
      "15      digital_literacy_meme    0.029818\n",
      "32              emotion_total    0.027679\n",
      "36              email_recency    0.021748\n",
      "51     email_seemed_different    0.018889\n",
      "31              emotion_worry    0.017314\n",
      "14      digital_literacy_wiki    0.015383\n",
      "17  digital_literacy_bookmark    0.013836\n",
      "27            emotion_nervous    0.012510\n",
      "16  digital_literacy_phishing    0.011946\n",
      "72             perceived_harm    0.011453\n",
      "0                age_category    0.009703\n",
      "19       digital_literacy_ssl    0.009309\n",
      "\n",
      "All models and predictor saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# === MODEL TRAINING ===\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Paths\n",
    "FEATURES_DIR = Path(\"data/features\")\n",
    "MODELS_DIR = Path(\"models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(FEATURES_DIR / \"wash_2021_ml_optimized.csv\")\n",
    "\n",
    "# Define features and targets\n",
    "X_features = [f for f in df.columns if f not in ['final_decision', 'actions_taken_clicked',\n",
    "                                                 'actions_taken_reported', 'actions_taken_deleted',\n",
    "                                                 'actions_taken_ignored', 'decision_confidence']]\n",
    "\n",
    "classification_targets = ['final_decision', 'actions_taken_clicked', \n",
    "                          'actions_taken_reported', 'actions_taken_deleted', 'actions_taken_ignored']\n",
    "regression_targets = ['decision_confidence']\n",
    "\n",
    "# Prepare features\n",
    "X = df[X_features].fillna(0)\n",
    "print(f\"Features: {X.shape[1]}, Samples: {X.shape[0]}\")\n",
    "\n",
    "# Model trainer\n",
    "def train_model(X, y, task='classification', name=''):\n",
    "    models = {\n",
    "        'rf': RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced', random_state=42) if task == 'classification'\n",
    "              else RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "        'lr_or_ridge': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)) if task == 'classification'\n",
    "            else ('model', Ridge(random_state=42))\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    scoring = 'accuracy' if task == 'classification' else 'r2'\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) if task == 'classification' else 5\n",
    "\n",
    "    best_model, best_score, best_name = None, -np.inf, None\n",
    "\n",
    "    for k, model in models.items():\n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "        mean_score = scores.mean()\n",
    "        print(f\"{name} | {k.upper()} | {scoring.upper()}={mean_score:.4f}\")\n",
    "        if mean_score > best_score:\n",
    "            best_model, best_score, best_name = model, mean_score, k\n",
    "\n",
    "    # Final train-test split for evaluation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y if task=='classification' else None,\n",
    "                                                        test_size=0.2, random_state=42)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    if task == 'classification':\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    else:\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        print(f\"Test R²: {r2_score(y_test, y_pred):.4f}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return best_model, best_score, best_name\n",
    "\n",
    "# Store results\n",
    "trained_models = {}\n",
    "model_info = {}\n",
    "\n",
    "# Classification\n",
    "for target in classification_targets:\n",
    "    if target in df.columns and df[target].nunique() > 1:\n",
    "        y = df[target].fillna(0).astype(int)\n",
    "        model, score, name = train_model(X, y, 'classification', target)\n",
    "        trained_models[target] = model\n",
    "        model_info[target] = {'type': 'classification', 'model': name, 'score': score}\n",
    "\n",
    "# Regression\n",
    "for target in regression_targets:\n",
    "    if target in df.columns and df[target].nunique() > 1:\n",
    "        y = df[target].fillna(0)\n",
    "        model, score, name = train_model(X, y, 'regression', target)\n",
    "        trained_models[target] = model\n",
    "        model_info[target] = {'type': 'regression', 'model': name, 'score': score}\n",
    "        \n",
    "# Show top features for all trained models\n",
    "print(\"\\n=== Top Feature Importances for Each Model ===\")\n",
    "\n",
    "for target, model in trained_models.items():\n",
    "    print(f\"\\n--- {target} ---\")\n",
    "    \n",
    "    if hasattr(model, \"feature_importances_\"):  # Random Forest\n",
    "        importances = model.feature_importances_\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': importances\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "        print(importance_df.head(15))\n",
    "        \n",
    "    elif hasattr(model, \"coef_\"):  # Logistic or Ridge Regression\n",
    "        coefs = model.coef_.flatten()\n",
    "        coef_df = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'coefficient': coefs\n",
    "        }).sort_values(by='coefficient', key=lambda x: abs(x), ascending=False)\n",
    "        print(coef_df.head(15))\n",
    "        \n",
    "    else:\n",
    "        print(\"No feature importance available for this model.\")\n",
    "\n",
    "# Feature importance for main model\n",
    "# def show_feature_importance(model, feature_names, top_n=15):\n",
    "#     if hasattr(model, 'feature_importances_'):\n",
    "#         importances = model.feature_importances_\n",
    "#     elif isinstance(model, Pipeline) and hasattr(model.named_steps['model'], 'coef_'):\n",
    "#         importances = np.abs(model.named_steps['model'].coef_[0])\n",
    "#     else:\n",
    "#         return\n",
    "#     fi = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "#     print(\"\\nTop Features:\\n\", fi.sort_values('importance', ascending=False).head(top_n))\n",
    "\n",
    "# if 'final_decision' in trained_models:\n",
    "#     print(\"\\nFeature importance for final_decision model:\")\n",
    "#     show_feature_importance(trained_models['final_decision'], X_features)\n",
    "\n",
    "# Save models & metadata\n",
    "for name, model in trained_models.items():\n",
    "    joblib.dump(model, MODELS_DIR / f\"wash_{name}_model.joblib\")\n",
    "\n",
    "# Save feature list and model info\n",
    "joblib.dump({\n",
    "    'features': X_features, \n",
    "    'models': model_info,\n",
    "    'feature_count': len(X_features)\n",
    "}, MODELS_DIR / \"wash_metadata.joblib\")\n",
    "\n",
    "# Create prediction class instead of function\n",
    "class WashPredictor:\n",
    "    def __init__(self, models, features):\n",
    "        self.models = models\n",
    "        self.features = features\n",
    "        self.model_info = model_info\n",
    "    \n",
    "    def __call__(self, input_data):\n",
    "        if isinstance(input_data, dict):\n",
    "            input_data = pd.DataFrame([input_data])\n",
    "        \n",
    "        # Ensure all features present\n",
    "        for f in self.features:\n",
    "            if f not in input_data.columns:\n",
    "                input_data[f] = 0\n",
    "        \n",
    "        input_data = input_data[self.features].fillna(0)\n",
    "        \n",
    "        predictions = {}\n",
    "        for target, model in self.models.items():\n",
    "            pred = model.predict(input_data)\n",
    "            \n",
    "            if self.model_info[target]['type'] == 'classification':\n",
    "                prob = model.predict_proba(input_data)[0][1] if hasattr(model, 'predict_proba') else None\n",
    "                predictions[target] = {\n",
    "                    'prediction': int(pred[0]), \n",
    "                    'probability': float(prob) if prob is not None else None\n",
    "                }\n",
    "            else:\n",
    "                predictions[target] = {'prediction': float(pred[0])}\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Save the predictor class with models embedded\n",
    "wash_predictor = WashPredictor(trained_models, X_features)\n",
    "joblib.dump(wash_predictor, MODELS_DIR / \"wash_predictor.joblib\")\n",
    "\n",
    "print(\"\\nAll models and predictor saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cypersona",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
